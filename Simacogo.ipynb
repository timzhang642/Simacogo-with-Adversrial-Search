{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC480 Project2: Simacogo\n",
    "<p>Yuxuan Zhang</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Human player goes first, using checker \"O\", play as MIN</p>\n",
    "<p>Agent goes second, using checker \"X\", play as MAX.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,state,action,depth,whos_move):\n",
    "        self.state = state\n",
    "        self.action = action # choice of move (row, column)\n",
    "        self.depth = depth # number of levels for the agent to search down the tree, should be an even number\n",
    "        self.whos_move = whos_move # 1 if it's human's move, -1 if its agent's move\n",
    "        self.best_score = None # best heuristic score for agent, which is agent score - human score, to be found later\n",
    "        self.children = [] # will include children nodes\n",
    "        self.best_child = None # the best child of the current node that maximize agent's gain\n",
    "        #self.add_children() # populate its children when the node is created\n",
    "    \n",
    "    # return T/F whether the game is over, that is, whether the board is full\n",
    "    def game_is_over(self):\n",
    "        if np.sum(self.state=='-') == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # return score if the same neighbor is found\n",
    "    def find_neighbor(self,row,col,checker,neighbor_type):\n",
    "        if row in range(0,9) and col in range(0,9):\n",
    "            if self.state[row,col] == checker:\n",
    "                return 1*neighbor_type\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # get score for both agent and human\n",
    "    # 2 points for every pair of pieces next to each other (up, down, left, right)\n",
    "    # 1 point for every pair of pieces diagonally next to each other\n",
    "    # heuristic score that evaluates both its score and try to hamstring opponent's best move\n",
    "    def get_score(self,checker,opponent_checker):\n",
    "            \n",
    "        self_score = 0 # board score that should be returned to front end\n",
    "        heuristic_score = 0 # heuristic score that's used to calculate best move\n",
    "        \n",
    "        # evaluate self_score based on current board\n",
    "        indices = np.nonzero(self.state==checker) # return row indices and column indices\n",
    "        indices = zip(indices[0],indices[1]) # turn into (row,col) index pairs\n",
    "        for row,col in indices:\n",
    "            # search its neighbors\n",
    "            # up\n",
    "            self_score += self.find_neighbor(row-1,col,checker,2)\n",
    "            # left\n",
    "            self_score += self.find_neighbor(row,col-1,checker,2)\n",
    "            # down\n",
    "            self_score += self.find_neighbor(row+1,col,checker,2)\n",
    "            # right\n",
    "            self_score += self.find_neighbor(row,col+1,checker,2)\n",
    "            # up left\n",
    "            self_score += self.find_neighbor(row-1,col-1,checker,1)\n",
    "            # up right\n",
    "            self_score += self.find_neighbor(row-1,col+1,checker,1)\n",
    "            # down left\n",
    "            self_score += self.find_neighbor(row+1,col-1,checker,1)\n",
    "            # down right\n",
    "            self_score += self.find_neighbor(row+1,col+1,checker,1)\n",
    "            \n",
    "        self_score /= 2 # remove the double counting effect\n",
    "            \n",
    "        # search opponent neighbors around the checker just dropped\n",
    "        row,col = self.action\n",
    "        # up\n",
    "        heuristic_score += self.find_neighbor(row-1,col,opponent_checker,2)\n",
    "        # left\n",
    "        heuristic_score += self.find_neighbor(row,col-1,opponent_checker,2)\n",
    "        # down\n",
    "        heuristic_score += self.find_neighbor(row+1,col,opponent_checker,2)\n",
    "        # right\n",
    "        heuristic_score += self.find_neighbor(row,col+1,opponent_checker,2)\n",
    "        # up left\n",
    "        heuristic_score += self.find_neighbor(row-1,col-1,opponent_checker,1)\n",
    "        # up right\n",
    "        heuristic_score += self.find_neighbor(row-1,col+1,opponent_checker,1)\n",
    "        # down left\n",
    "        heuristic_score += self.find_neighbor(row+1,col-1,opponent_checker,1)\n",
    "        # down right\n",
    "        heuristic_score += self.find_neighbor(row+1,col+1,opponent_checker,1)\n",
    "        heuristic_score *= 0.5 # weight less on blocking the opponent, weight more on earning its own score\n",
    "            \n",
    "        return self_score,heuristic_score\n",
    "                    \n",
    "    def add_best_child(self,col):\n",
    "        if self.depth > 0 and not self.game_is_over():\n",
    "            row = len(self.state[:,col][self.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >=0:\n",
    "                new_state = self.state.copy()\n",
    "                if self.whos_move == 1:\n",
    "                    new_state[row,col] = 'X'\n",
    "                else:\n",
    "                    new_state[row,col] = 'O'\n",
    "                self.best_child = Node(state=new_state,action=(row,col),depth=self.depth-1,whos_move=-self.whos_move)\n",
    "\n",
    "# given a node, grow the adversarial tree and return agent's action\n",
    "def minimax(node, alpha, beta):\n",
    "    \n",
    "    temp_score_list = [] # store the heuristic score for states explored\n",
    "    \n",
    "    # when the search depth is reached or the game board is full, return agent score at terminal nodes\n",
    "    if node.depth == 0 or node.game_is_over():\n",
    "        print node.state,'\\n'\n",
    "\n",
    "        agent_self_score,agent_heuristic_score = node.get_score(checker='X',opponent_checker='O')\n",
    "        human_self_score,human_heuristic_score = node.get_score(checker='O',opponent_checker='X')\n",
    "        return agent_self_score + agent_heuristic_score - human_self_score - human_heuristic_score\n",
    "    \n",
    "    # when the upcoming is agent's move, which plays as MAX\n",
    "    if node.whos_move == 1:\n",
    "        agent_best_score = -10000\n",
    "        \n",
    "        # search for agent's moves\n",
    "        for col in range(9):\n",
    "            row = len(node.state[:,col][node.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >= 0:\n",
    "                new_state = node.state.copy()\n",
    "                new_state[row,col] = 'X'\n",
    "                child = Node(state=new_state,action=(row,col),depth=node.depth-1,whos_move=-node.whos_move)\n",
    "                node.children.append(child) \n",
    "                temp_score = minimax(node=child, alpha=agent_best_score, beta=beta)\n",
    "                temp_score_list.append(temp_score)\n",
    "                \n",
    "                if temp_score > agent_best_score:\n",
    "                    agent_best_score = temp_score\n",
    "                    node.best_score = agent_best_score\n",
    "                    node.add_best_child(col)\n",
    "                \n",
    "                # pruning\n",
    "                if agent_best_score > beta:\n",
    "                    return agent_best_score\n",
    "                    \n",
    "                alpha = max(alpha,agent_best_score)\n",
    "        return node.best_score\n",
    "    \n",
    "    # when the upcoming is human's move, which plays as MIN\n",
    "    else:\n",
    "        human_best_score = 10000\n",
    "        \n",
    "        # search for human's moves\n",
    "        for col in range(9):\n",
    "            row = len(node.state[:,col][node.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >= 0:\n",
    "                new_state = node.state.copy()\n",
    "                new_state[row,col] = 'O'\n",
    "                child = Node(state=new_state,action=(row,col),depth=node.depth-1,whos_move=-node.whos_move)\n",
    "                node.children.append(child) \n",
    "                temp_score = minimax(node=child, alpha=alpha, beta=human_best_score)\n",
    "                temp_score_list.append(temp_score)\n",
    "                \n",
    "                if temp_score < human_best_score:\n",
    "                    human_best_score = temp_score\n",
    "                    node.best_score = human_best_score\n",
    "                    node.add_best_child(col)\n",
    "                    \n",
    "                # pruning\n",
    "                if human_best_score < alpha:\n",
    "                    return human_best_score\n",
    "                    \n",
    "                beta = min(beta,human_best_score)\n",
    "        return node.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', 'O', '-', '-', '-', '-']], \n",
       "      dtype='|S21')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = np.zeros([9,9])\n",
    "init_state=np.where(init_state==0.,'-',0)\n",
    "init_state[8,4] = 'O'\n",
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a search tree based on current state\n",
    "# action is the column that human player picked, assume to be 4\n",
    "# who's move=1 if upcoming is agent's turn to move, -1 if upcoming is human's turn\n",
    "current_node = Node(state=init_state,action=(8,4),depth=1,whos_move=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['X' '-' '-' '-' 'O' '-' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' 'X' '-' '-' 'O' '-' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' 'X' '-' 'O' '-' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' 'O' '-' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'X' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'O' '-' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'O' 'X' '-' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'O' '-' 'X' '-' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'O' '-' '-' 'X' '-']] \n",
      "\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' 'O' '-' '-' '-' 'X']] \n",
      "\n",
      "\n",
      "NOW: expected_agent_score 1.0 best_agent_action (8, 3)\n",
      "State after agent's best move:\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' 'O' '-' '-' '-' '-']]\n"
     ]
    }
   ],
   "source": [
    "alpha = -10000 # best score for Agent, which wants to maximize it\n",
    "beta = 10000 # best score for Human, which wants to minimize it\n",
    "\n",
    "# return the best heuristic (score,action) for agent, action refers to which column to choose\n",
    "expected_agent_score = minimax(node=current_node, alpha=alpha, beta=beta) \n",
    "print '\\nNOW: expected_agent_score',expected_agent_score,'best_agent_action',current_node.best_child.action\n",
    "print \"State after agent's best move:\"\n",
    "print current_node.best_child.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
