{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC480 Project2: Simacogo\n",
    "<p>Yuxuan Zhang</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Human player goes first, using checker \"O\", play as MIN</p>\n",
    "<p>Agent goes second, using checker \"X\", play as MAX.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,state,action,depth,whos_move):\n",
    "        self.state = state\n",
    "        self.action = action # choice of column 1-9\n",
    "        self.depth = depth # number of levels for the agent to search down the tree, should be an even number\n",
    "        self.whos_move = whos_move # 1 if it's human's move, -1 if its agent's move\n",
    "        self.best_score = None # best heuristic score for agent, which is agent score - human score, to be found later\n",
    "        self.children = [] # will include children nodes\n",
    "        self.best_child = None # the best child of the current node that maximize agent's gain\n",
    "        #self.add_children() # populate its children when the node is created\n",
    "    \n",
    "    # return T/F whether the game is over, that is, whether the board is full\n",
    "    def game_is_over(self):\n",
    "        if np.sum(self.state=='-') == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # return score if the same neighbor is found\n",
    "    def find_neighbor(self,row,col,checker,neighbor_type):\n",
    "        try:\n",
    "            if self.state[row,col] == checker:\n",
    "                return 1*neighbor_type\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    # get score for both agent and human\n",
    "    # 2 points for every pair of pieces next to each other (up, down, left, right)\n",
    "    # 1 point for every pair of pieces diagonally next to each other\n",
    "    def get_score(self,checker):\n",
    "        score = 0\n",
    "        indices = np.nonzero(self.state==checker) # return row indices and column indices\n",
    "        indices = zip(indices[0],indices[1]) # turn into (row,col) index pairs\n",
    "        for row,col in indices:\n",
    "            # up\n",
    "            score += self.find_neighbor(row-1,col,checker,2)\n",
    "            # left\n",
    "            score += self.find_neighbor(row,col-1,checker,2)\n",
    "            # down\n",
    "            score += self.find_neighbor(row+1,col,checker,2)\n",
    "            # right\n",
    "            score += self.find_neighbor(row,col+1,checker,2)\n",
    "            # up left\n",
    "            score += self.find_neighbor(row-1,col-1,checker,1)\n",
    "            # up right\n",
    "            score += self.find_neighbor(row-1,col+1,checker,1)\n",
    "            # down left\n",
    "            score += self.find_neighbor(row+1,col-1,checker,1)\n",
    "            # down right\n",
    "            score += self.find_neighbor(row+1,col+1,checker,1)\n",
    "\n",
    "        score /= 2 # remove the double count effect\n",
    "        return score\n",
    "                    \n",
    "    def add_best_child(self,col):\n",
    "        if self.depth > 0 and not self.game_is_over():\n",
    "            row = len(self.state[:,col][self.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >=0:\n",
    "                new_state = self.state.copy()\n",
    "                if self.whos_move == 1:\n",
    "                    new_state[row,col] = 'X'\n",
    "                else:\n",
    "                    new_state[row,col] = 'O'\n",
    "                self.best_child = Node(state=new_state,action=col,depth=self.depth-1,whos_move=-self.whos_move)\n",
    "\n",
    "# given a node, grow the adversarial tree and return agent's action\n",
    "def minimax(node, alpha, beta):\n",
    "    #print 'node.depth:',node.depth,'node whos_move:',node.whos_move\n",
    "    \n",
    "    # when the search depth is reached or the game board is full, return agent score at terminal nodes\n",
    "    if node.depth == 0 or node.game_is_over():\n",
    "        # heurstic score is the difference between agent's and human's score\n",
    "        agent_score = node.get_score('X')\n",
    "        human_score = node.get_score('O')\n",
    "        node.score = agent_score - human_score\n",
    "        return node.score\n",
    "    \n",
    "    # when the upcoming is agent's move, which plays as MAX\n",
    "    if node.whos_move == 1:\n",
    "        agent_best_score = -10000\n",
    "        \n",
    "        # search for agent's moves\n",
    "        for col in range(9):\n",
    "            row = len(node.state[:,col][node.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >=0:\n",
    "                new_state = node.state.copy()\n",
    "                new_state[row,col] = 'X'\n",
    "                child = Node(state=new_state,action=col,depth=node.depth-1,whos_move=-node.whos_move)\n",
    "                node.children.append(child) \n",
    "                temp_score = minimax(node=child, alpha=agent_best_score, beta=beta)\n",
    "                if temp_score > agent_best_score:\n",
    "                    agent_best_score = temp_score\n",
    "                    node.best_score = agent_best_score\n",
    "                    node.add_best_child(col)\n",
    "                \n",
    "                # pruning\n",
    "                if agent_best_score > beta:\n",
    "                    return agent_best_score\n",
    "                    \n",
    "                alpha = max(alpha,agent_best_score)\n",
    "                #print 'Agent temp_score:',temp_score,'agent_best_score:',agent_best_score\n",
    "        #print node.state\n",
    "        #print 'Upcoming Agent move, depth:',node.depth,'whos_move next:',-node.whos_move,'best score for agent:',node.best_score,'best action for agent to choose:',node.best_child.action,'\\n'\n",
    "        return node.best_score\n",
    "    \n",
    "    # when the upcoming is human's move, which plays as MIN\n",
    "    else:\n",
    "        human_best_score = 10000\n",
    "        \n",
    "        # search for human's moves\n",
    "        for col in range(9):\n",
    "            row = len(node.state[:,col][node.state[:,col]=='-'])-1 # row index for dropping the checker\n",
    "            if row >=0:\n",
    "                new_state = node.state.copy()\n",
    "                new_state[row,col] = 'O'\n",
    "                child = Node(state=new_state,action=col,depth=node.depth-1,whos_move=-node.whos_move)\n",
    "                node.children.append(child) \n",
    "                temp_score = minimax(node=child, alpha=alpha, beta=human_best_score)\n",
    "                if temp_score < human_best_score:\n",
    "                    human_best_score = temp_score\n",
    "                    node.best_score = human_best_score\n",
    "                    node.add_best_child(col)\n",
    "                    \n",
    "                # pruning\n",
    "                if human_best_score < alpha:\n",
    "                    return human_best_score\n",
    "                    \n",
    "                beta = min(beta,human_best_score)\n",
    "                #print 'Human temp_score:',temp_score,'human_best_score:',human_best_score\n",
    "        #print node.state\n",
    "        #print 'Upcoming Human move, depth:',node.depth,'whos_move next:',-node.whos_move,'best score for human:',node.best_score,'best action for human to choose:',node.best_child.action,'\\n'\n",
    "        return node.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', 'X', '-', '-', '-', '-', '-'],\n",
       "       ['-', '-', '-', 'O', 'O', '-', '-', '-', '-']], \n",
       "      dtype='|S21')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = np.zeros([9,9])\n",
    "init_state=np.where(init_state==0.,'-',0)\n",
    "init_state[8,[3,4]] = 'O'\n",
    "init_state[7,3] = 'X'\n",
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a search tree based on current state\n",
    "# action is the column that human player picked, assume to be 4\n",
    "# depth should be an even number\n",
    "# who's move=1 if upcoming is agent's turn to move, -1 if upcoming is human's turn\n",
    "current_node = Node(state=init_state,action=4,depth=4,whos_move=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOW: expected_agent_score -2 best_agent_action 4\n",
      "State after agent's best move:\n",
      "[['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' 'X' '-' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' 'O' '-' '-' '-' '-']]\n"
     ]
    }
   ],
   "source": [
    "alpha = -10000 # best score for Agent, which wants to maximize it\n",
    "beta = 10000 # best score for Human, which wants to minimize it\n",
    "\n",
    "# return the best heuristic (score,action) for agent, action refers to which column to choose\n",
    "expected_agent_score = minimax(node=current_node, alpha=alpha, beta=beta) \n",
    "print '\\nNOW: expected_agent_score',expected_agent_score,'best_agent_action',current_node.best_child.action\n",
    "print \"State after agent's best move:\"\n",
    "print current_node.best_child.state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
